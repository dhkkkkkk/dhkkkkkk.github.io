---
title: 机器学习的一些概念
date: 2024-07-17 14:17:08
tags: 深度学习
---

# 监督学习

监督学习是从**对应了某种特定标签的的训练数据（例如房价预测中的输入里，一个面积对应一个的价格）**中学习并建立模型，然后基于该模型预测未知的样本。其中，模型的**输入是某个样本数据的特征**，而函数的输出是**与该样本相对应的标签**。监督学习算法则负责预测输入到输出之间的**映射关系**

例如：输入为语音，输出为文字，则该监督学习的应用即为语音识别

分类：

* 回归算法(regression)
* 分类算法(classfication)。分类预测的是一个小的、有限的一套阐述类别（输出大多为离散、有限的）

## 线性回归模型（linear regression）

* 训练集（training set）：用于训练模型的已知数据**集合**
* 输入变量x(input/feature)：训练集中的输入
* 输出变量y (output/target)：训练集中的输出
* 模型：用于预测的函数  
* 预测y-hat(^)（prediction）：模型的输出，**注意区分y和y-hat**

### 代价函数（cost function）or损失函数（loss）

用于评估模型输出和训练数据的拟合程度

因此代价函数输出应尽可能**小**

#### 平方误差代价函数（squared error cost function）

{% asset_img 2.JPG This is an image %} 

模型参数w，b会影响代价函数的输出，而找到当前训练集下使代价函数输出最小的参数w，b，即可找到对当前训练集最适合的线性回归模型

{% asset_img 3.jpg This is an image %} 

<u>对于使用平方误差代价函数的线性回归中总会出现弓形、吊床形的图像</u>

即只有一个最小值

### 梯度下降算法（gradient descent）

可用于尝试最小化任何函数

{% asset_img 4.jpg This is an image %} 

* 参数w、b必须同时更新，而不算将更新后的w用于b的更新中

* α：学习率，学习率过大会导致代价函数无法收敛，甚至效果越来越差

* 由式可以看出，**梯度下降算法只 能找到代价函数中的某个极小值**，而对于找到的极小值则与w，b在梯度下降算法中的初始值有关

{% asset_img 5.jpg This is an image %}

由于两个参数的梯度下降式是互相独立的，因此对w或b的更新可以视作，另一个参数固定。再结合梯度下降算式可以看出该算法其实是利用了斜率的正负特性进行找极小值的过程 

## 分类算法

### 逻辑回归算法（非回归算法）logistic regression



# 非监督学习

非监督学习算法的输入样本**不对应某种特定的标签**，而是**自动**地从样本中学习以找到某种结构、模式......

分类：

* 聚类算法(clustering)：例如从数万篇文章中找到相似词（**非特定**）并聚类
* 异常检测（anomaly detection）
* 降维（dimensionality reduction） 

吴恩达の小练习：（其中两个为监督两个为非监督）

{% asset_img 1.jpg This is an image %} 

# jupyter Notebook

```
python -m pip install --upgrade pip
pip install jupyter notebook
```

剩下的一些操作lab1文件写的很清楚，忘了就去看看

# 参数归一化（normalization）

由于各参数（特征）的变化范围、大小、单位不同，需要先进行归一化处理后才输入到模型

## 平均归一化（mean normalization）

## Z-score normalization