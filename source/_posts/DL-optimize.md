---
title: 神经网络的优化手段
date: 2025-10-05 15:19:37
tags: 深度学习
mathjax: true
---

# 网络优化

网络优化可以从以下四个方向展开：

* 使用更有效的优化算法
* 使用更好的参数初始化、数据预处理方法
* 修改网络结构（激活函数、残差连接、归一化）
* 使用更好的超参数优化方法

# 优化算法

## minibatch与梯度积累

一般而言，batchsize较大时会获得较稳定的训练结果（因此可以设置较大学习率）。但batchsize会被GPU显存所限制，因此可以通过梯度累积的方式，使用较小的batchsize模拟大batchsize的训练效果：

```python
batch_size = 16
accumulation_steps = 4 
for i, (inputs, labels) in enumerate(dataloader):
    # 前向传播
    outputs = model(inputs)
    loss = criterion(outputs, labels)
    loss = loss / accumulation_steps  # 损失除以累积步数
    
    # 反向传播
    loss.backward()
    
    # 每 accumulation_steps 次更新一次权重
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

该代码通过16的batchsize模拟了64batchsize的训练效果，具体其实就是每4个batch才进行一层更新权重。

对于pytorch，在计算了每个batch中各**样本**的loss后，会对其进行**求均值处理**，因此上述代码一次循环中的loss中包括了16个loss的均值，因此其反向传播计算的梯度也是一个**平均梯度**；而pytorch中，如果不手动对梯度进行清零（zero_grad()），其会自动在得到新梯度时**求和**，因此上述代码需要使用：

`loss = loss / accumulation_steps`

对每个batch的平均loss根据accumulation_steps再次进行平均处理，最后每4个batch后生成的loss才是batchsize=64时得到的平均loss，其梯度也为4个batch的平均梯度

**如果不进行均值处理，则等同于学习率被放大同等倍数**，会导致训练不稳定

## batchsize影响

对于引入了batch进行局部梯度优化的神经网络训练（大多数训练都采取该策略），batchsize越大会令每个batch的梯度方差越小，从而使训练更加稳定，举个例子：

假设一个针对猫狗的二分类图像识别，当batchsize较小时，一个batch可能大部分由猫组成，而另一个batch可能又大部分由狗组成，这将导致这**两个batch的梯度大概率完全不同，甚至是相反的方向**（由之前对反向传播的推到中可得知某层参数的偏导与之前层的激活函数梯度与该层输入值有关），最终**导致参数更新左右震荡**

## 学习率调整

### 学习率衰减

无需多言，训练后期为了避免在最优点附近来回震荡，要对学习率进行衰减处理，常见操作有：

* 分段衰减
* 逆时衰减
* 指数衰减
* 余弦衰减

其中逆时、指数衰减呈类exp(-x)形状，余弦衰减呈余弦（0-pi）形状，因此在训练前期前者衰减会大于后者，当然，各衰减方法都有可调参数可具体控制衰减情况，不用过于纠结。

### 学习率预热

由于神经网络在训练最初参数是随机初始化的，因此在刚开始训练时梯度会较大，而学习率最开始也较大，容易出现训练不稳定的情况。因此可以在最初几轮采用较小学习率，让学习率在一定预热回合内逐渐上升为初始学习率。（在SNN中貌似很常用，在常规神经网络中貌似不是很必要）

## 梯度估计修正（优化器选择）

之前我们说到现在大部分神经网络训练都使用batch进行**梯度估计**以代替整个训练集上的真正梯度，这样做会导致每个batch的平均梯度具有一定随机性（与全局最优梯度不一致），因此出现了**许多通过使用一段时间（多个batch）内的平均梯度以代替单个batch梯度的方法**缓解梯度估计的随机性。

**<u>这在pytorch中直接体现在使用不同的优化器</u>**，这也就是为什么对梯度的管理（除了计算）都是通过优化器类的方法实现(如step，zero_grad)

而SGD优化器则是直接使用当前batch的估计梯度进行更新，最为简单，但这并不意为着SGD不好，相反，SGD在精细调优后和训练后期可以实现更稳定、精准的效果。

另一个常见的Adam优化器则使用了**动量法+自适应学习率**的方式进行梯度估计修正，动量法可以考虑到多个batch的平均梯度，降低梯度估计随机性；同时Adam算法使用了自适应学习率，使其在固定学习率下也可以有较好训练效果（但最好还是手动添加一个学习率衰减）

## 梯度爆炸问题

当某次反向传播时梯度突然暴增会导致更新参数时反而远离最优点，因此可以采用一些方法避免：

* 梯度裁剪：设置梯度阈值，当超过阈值时按比例缩小至阈值
* 使用归一化稳定每一层神经网络的输出

# 参数初始化与数据预处理

这一块感觉没啥好说的，就只在这里列出，不展开说了，个人认为除了数据输入归一化外其他作用不大。

## 归一化与标准化

# 归一化处理
