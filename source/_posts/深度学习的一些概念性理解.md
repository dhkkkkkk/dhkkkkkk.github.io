---
title: 深度学习的一些概念性理解
date: 2024-10-22 16:46:18
tags: 深度学习
---

# 激活函数

## 为什么要使用激活函数

如果没有激活函数，神经网络中将只存在大量的`权重*输入`（还有偏置）的线性叠加，无法拟合出非线性函数的效果。当引入非线性的激活函数后，神经网络便可以逼近任何线性、非线性函数

## 常见激活函数

### sigmoid函数

指一类S型函数

* 优点：logistic函数可以将输出归一到(0,1)，而Tanh函数可以处理负数输入，因此Tanh会更常用一些
* 缺点：**在两个饱和处，其斜率变化较缓，导致w的变化对loss的影响较小，最终导致w的更新变慢**（梯度消失）（我感觉这样说好理解一点）

### ReLU函数

引入了负数全部置0的非线性元素，且其正部本身为线性，斜率为1，不会出现梯度消失问题

缺点：但如果参数在一次不恰当的更新后，导致某一个层输入到某个神经元的值为0时，**该神经元输出到后面所有神经元的值全为0，此时该神经元的w无论怎么变化，输出到下一层所有神经元的值都为0（梯度为0），导致该神经元前的参数都无法更新**，出现dead relu

# 反向传播（BP）

## 参数学习

以交叉熵损失函数为例，假设y为one-hot向量（仅标签位置为1，其他0），则可以延伸至多分类问题，且其输出仅与标签位置处的y^相关，y^越小，损失函数越大(y^<1)

{% asset_img 1.jpg This is an image %} 

由于y^是W（权重矩阵）和偏置的函数，因此Loss函数也为W和b的函数，通过优化迭代他们的值即可接近Loss的最小值，而要降低Loss，通常是借助各w的梯度实现

## 反向传播算法

当一次正向传播后，Loss值已经确定，此时Loss对所有w的偏导都已确定，因此计算出这些偏导即可完成一次梯度下降。如果从开头通过链式法则对所有w逐一计算效率较低。但由链式法则又可得：

**loss对某一w的偏导可通过下一层的所有w偏导计算而得，因此从输出层对w偏导进行反向计算效率更高**

# 注意力机制

## 向量点乘的几何意义

向量**x**在向量**y**方向上的投影再与向量**y**的乘积，能够反应两个向量的相似度。<u>向量点乘结果越大，两个向量越相似。</u>

## 作用

注意力机制通过计算输入序列中每个位置（时间步）的权重，来决定每个时间步的重要性。权重越高，说明该时间步包含的信息越关键，模型就会更“关注”该部分。

与之相比，卷积只能使输入局部相关，而全连接层则计算量大，**注意力机制则可以根据整个输入序列实时调整权重**

