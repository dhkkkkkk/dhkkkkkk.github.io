---
title: LLMå­¦ä¹ 
date: 2025-11-21 09:27:07
tags: æ·±åº¦å­¦ä¹ 
---

æœ¬ç« æ˜¯åœ¨æœ‰ä¸€å®šæ·±åº¦å­¦ä¹ åŸºç¡€ï¼Œç†Ÿæ‚‰attentionå’Œtransformerç»“æ„çš„åŸºç¡€ä¸Šå†™çš„

# Transformer

## é€šç”¨æ¶æ„

* encoderç”¨äºæ¥æ”¶æºè¾“å…¥å¹¶æå–ç‰¹å¾ï¼ˆä¾‹å¦‚ç¿»è¯‘ä»»åŠ¡ä¸­çš„å¾…ç¿»è¯‘åŸæ–‡ï¼‰

* decoderé€šè¿‡encoderè¾“å‡ºçš„ç‰¹å¾å’Œä¹‹å‰çš„è¾“å‡ºå†…å®¹ï¼Œç”Ÿæˆä¸‹ä¸€ä¸ªè¾“å‡º

  ä¾‹å¦‚ï¼Œå½“è¦å°†"I love you"ç¿»è¯‘ä¸ºä¸­æ–‡æ—¶ï¼Œencoderæ¥æ”¶çš„ä¾¿æ˜¯"I love you"å…¨æ–‡ï¼Œdecoderåˆ™æ˜¯æŒ‰é¡ºåºä¾æ¬¡ç”Ÿæˆâ€œæˆ‘â€ï¼Œâ€œçˆ±â€ï¼Œâ€œä½ â€ï¼Œè€Œå½“ç”Ÿæˆâ€œçˆ±"æ—¶ï¼Œdecoderä¼šå°†â€æˆ‘â€ä¸å¯¹â€I love youâ€œçš„ç‰¹å¾è¿›è¡Œattentionè®¡ç®—ï¼Œåœ¨è®­ç»ƒä¼˜å¼‚çš„æƒ…å†µä¸‹ï¼Œâ€loveâ€œè¿™ä¸ªè°“è¯­ä¼šè¢«é‡ç‚¹å…³æ³¨ï¼ˆè¢«è®¤ä¸ºæ˜¯ä¸‹ä¸€ä¸ªè¢«ç¿»è¯‘çš„å¯¹è±¡ï¼‰ï¼Œdecoderä¼šç”Ÿæˆä¸€ä¸ªåŸºäºâ€Iâ€œå’Œâ€youâ€œçš„â€loveâ€œæ·±å±‚è¯­ä¹‰è¡¨è¾¾ï¼Œåœ¨decoderçš„æœ€åï¼Œè¯¥è¯­ä¹‰è¡¨è¾¾ä¼šä¸ä¸­æ–‡â€çˆ±â€œçš„åŒ¹é…åº¦æœ€é«˜

## attentionçš„æŠ½è±¡ç†è§£

è¿˜æ˜¯ä»¥â€I love youâ€œåˆ°â€æˆ‘çˆ±ä½ â€œçš„ç¿»è¯‘ä»»åŠ¡ä¸¾ä¾‹ï¼š

* encoderçš„è‡ªæ³¨æ„åŠ›ï¼šåœ¨æ¨¡å‹é€å­—ç¿»è¯‘ä¸­æ–‡çš„è¿‡ç¨‹ä¸­ï¼Œencoderåªè®¡ç®—ä¸€æ¬¡ã€‚encoderä¸»è¦æ˜¯ä¸ºäº†å¾—åˆ°è‹±æ–‡ä¸­â€œI love youâ€çš„è¯­ä¹‰å…³ç³»ï¼Œä¾‹å¦‚ä¸»è°“å®¾ç»“æ„ï¼Œæˆ–è€…â€œè¿™æ˜¯ä¸€ä¸ªè¡¨è¾¾æƒ…æ„Ÿçš„è¯­å¥â€œç­‰
* decoderçš„è‡ªæ³¨æ„åŠ›ï¼šå½“å·²å®Œæˆâ€æˆ‘â€œçš„ç¿»è¯‘æ—¶ï¼Œç°åœ¨decoderçš„è¾“å…¥åˆ™æ˜¯â€æˆ‘â€œï¼Œåœ¨è‡ªæ³¨æ„åŠ›è®¡ç®—ä¸­ï¼Œdecoderåˆ™ä¼šç”Ÿæˆä¸­æ–‡â€æˆ‘â€œçš„è¯­ä¹‰
* decoderçš„äº¤å‰æ³¨æ„åŠ›ï¼šç”±äºâ€æˆ‘â€œè¿™ä¸ªä¸­æ–‡è¯­ä¹‰ä¸­ç¼ºå¤±è°“è¯­ï¼Œå› æ­¤<u>åœ¨Qä¸Kçš„è®¡ç®—ä¸­</u>ï¼Œâ€loveâ€œè¿™ä¸ªè¡¨ç¤ºè°“è¯­çš„è¯è¯­ä¼šå¾—åˆ°æ›´é«˜çš„å…³æ³¨åº¦ï¼ˆåªæ˜¯ä¸¾ä¾‹ï¼Œå®é™…ä¸Šä¸åªæœ‰è¿™ä¸€ä¸ªåŸå› ï¼Œä½†å…·ä½“è®­ç»ƒè¿‡ç¨‹å…¶å®æˆ‘ä»¬äººç±»æ˜¯éš¾ä»¥ç†è§£çš„ï¼‰ï¼›å°†å…³æ³¨åº¦è¿›è¡Œsoftmaxå<u>ä¸Vç›¸ä¹˜</u>ï¼Œæœ€ç»ˆä¼šè¾“å‡ºä¸€ä¸ª**ä¸"love"å¼ºç›¸å…³çš„"I love you"çš„æ·±å±‚è¯­ä¹‰è¡¨è¾¾**ï¼ˆå¯ä»¥ç†è§£ä¸ºï¼šdecoderå½“å‰éœ€è¦ç¿»è¯‘â€œloveâ€ï¼Œå¹¶ä¸”è€ƒè™‘äº†â€œIâ€å’Œâ€œyouâ€åœ¨å…¨æ–‡ä¸­çš„å«ä¹‰ï¼‰ã€‚
* decoderå°¾éƒ¨ï¼šçº¿æ€§å±‚å’Œsoftmaxæœ€åä¼šåŸºäºä¹‹å‰çš„è¯­ä¹‰è¡¨è¾¾ï¼Œè¾“å‡ºä¸€ä¸ªä¸ä¸­æ–‡â€œçˆ±â€ç›¸ä¼¼åº¦æœ€é«˜çš„embedding

## è¯­è¨€æ¨¡å‹ç±»å‹

è¯­è¨€æ¨¡å‹é€šå¸¸åˆ†ä¸ºä¸‰ç§æ¶æ„ç±»åˆ«ï¼š

* **ä»…ç¼–ç å™¨æ¨¡å‹**ï¼ˆå¦‚ BERTï¼‰ï¼šè¿™äº›æ¨¡å‹ä½¿ç”¨åŒå‘æ–¹æ³•æ¥ç†è§£æ¥è‡ªä¸¤ä¸ªæ–¹å‘çš„ä¸Šä¸‹æ–‡ã€‚å®ƒä»¬æœ€é€‚åˆéœ€è¦æ·±å…¥ç†è§£æ–‡æœ¬çš„ä»»åŠ¡ï¼Œå¦‚**åˆ†ç±»ã€å‘½åå®ä½“è¯†åˆ«å’Œé—®ç­”**ã€‚
* **ä»…è§£ç å™¨æ¨¡å‹**ï¼ˆå¦‚ GPTã€Llamaï¼‰ï¼šè¿™äº›æ¨¡å‹ä»å·¦åˆ°å³å¤„ç†æ–‡æœ¬ï¼Œç‰¹åˆ«æ“…é•¿**æ–‡æœ¬ç”Ÿæˆ**ä»»åŠ¡ã€‚å®ƒä»¬å¯ä»¥æ ¹æ®æç¤ºå®Œæˆå¥å­ã€å†™æ–‡ç« ï¼Œç”šè‡³ç”Ÿæˆä»£ç ã€‚
* **ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹**ï¼ˆå¦‚ T5ã€BARTï¼‰ï¼šè¿™äº›æ¨¡å‹ç»“åˆäº†ä¸¤ç§æ–¹æ³•ï¼Œä½¿ç”¨ç¼–ç å™¨ç†è§£è¾“å…¥ï¼Œä½¿ç”¨è§£ç å™¨ç”Ÿæˆè¾“å‡ºã€‚å®ƒä»¬åœ¨åºåˆ—åˆ°åºåˆ—ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¦‚**ç¿»è¯‘ã€æ‘˜è¦å’Œé—®ç­”**ã€‚

è€Œè¿™äº›æ¨¡å‹é€šå¸¸æœ‰ä¸¤ç§è®­ç»ƒæ–¹æ³•ï¼š

1. **æ©ç è¯­è¨€å»ºæ¨¡ï¼ˆMLMï¼‰**ï¼šç”±åƒ BERT è¿™æ ·çš„ç¼–ç å™¨æ¨¡å‹ä½¿ç”¨ï¼Œè¿™ç§æ–¹æ³•éšæœºæ©ç›–è¾“å…¥ä¸­çš„ä¸€äº›è¯å…ƒï¼Œå¹¶è®­ç»ƒæ¨¡å‹æ ¹æ®å‘¨å›´çš„ä¸Šä¸‹æ–‡é¢„æµ‹åŸå§‹è¯å…ƒã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åŒå‘ä¸Šä¸‹æ–‡ï¼ˆåŒæ—¶å…³æ³¨è¢«æ©ç›–è¯è¯­ä¹‹å‰å’Œä¹‹åçš„è¯è¯­ï¼‰ã€‚
2. **å› æœè¯­è¨€å»ºæ¨¡ï¼ˆCLMï¼‰**ï¼šç”±åƒ GPT è¿™æ ·çš„è§£ç å™¨æ¨¡å‹ä½¿ç”¨ï¼Œè¿™ç§æ–¹æ³•æ ¹æ®åºåˆ—ä¸­æ‰€æœ‰ä¹‹å‰çš„è¯å…ƒæ¥é¢„æµ‹ä¸‹ä¸€ä¸ªè¯å…ƒã€‚æ¨¡å‹åªèƒ½ä½¿ç”¨å·¦ä¾§ï¼ˆä¹‹å‰çš„è¯å…ƒï¼‰çš„ä¸Šä¸‹æ–‡æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªè¯å…ƒã€‚

## éŸ³é¢‘Transformer

### æ¨¡å‹è¾“å…¥æ ¼å¼

* æ–‡æœ¬è¾“å…¥ï¼šé€šå¸¸å‡ºç°åœ¨æ–‡æœ¬åˆ°è¯­éŸ³çš„ä»»åŠ¡ä¸­ï¼ˆTTSï¼‰ï¼Œä¸åŸå§‹Transformeræˆ–ä»»ä½•å…¶ä»–NLPæ¨¡å‹çš„å·¥ä½œæ–¹å¼ç›¸åŒï¼š**é¦–å…ˆå¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°åŒ–**ï¼ˆtokenizationï¼‰ï¼Œå¾—åˆ°ä¸€ç³»åˆ—æ–‡æœ¬æ ‡è®°ã€‚ç„¶åå°†æ­¤åºåˆ—**é€šè¿‡è¾“å…¥åµŒå…¥å±‚**ï¼Œå°†æ ‡è®°è½¬æ¢ä¸º512ç»´å‘é‡ã€‚ç„¶åå°†è¿™äº›åµŒå…¥å‘é‡ä¼ é€’åˆ°Transformerç¼–ç å™¨ä¸­ã€‚
* æ³¢å½¢è¾“å…¥ï¼š**Wav2Vec2**å’Œ**HuBERT**ä¸€ç±»çš„æ¨¡å‹ç›´æ¥ä½¿ç”¨éŸ³é¢‘æ³¢å½¢ä½œä¸ºæ¨¡å‹çš„è¾“å…¥ã€‚æˆ‘ä»¬é¦–å…ˆå°†åŸå§‹æ³¢å½¢**æ ‡å‡†åŒ–ä¸ºé›¶å‡å€¼å’Œå•ä½æ–¹å·®çš„åºåˆ—**ï¼Œè¿™æœ‰åŠ©äºæ ‡å‡†åŒ–ä¸åŒéŸ³é‡ï¼ˆæŒ¯å¹…ï¼‰çš„éŸ³é¢‘æ ·æœ¬ã€‚å¯¹äºè¿™ç±»æ¨¡å‹ï¼Œencoderå‰ä¸€èˆ¬ä¼šç”¨ä¸€ä¸ªå°å‹CNNè¿›è¡Œä¸‹é‡‡æ ·å’Œæå–å±€éƒ¨ç‰¹å¾ï¼Œå‡å°‘åºåˆ—é•¿åº¦
* æ—¶é¢‘è°±è¾“å…¥ï¼šè€æœ‹å‹äº†ï¼Œé€šè¿‡æ—¶é¢‘åŸŸè½¬åŒ–å¯ä»¥å¤§å¹…ç¼©å‡æ ·æœ¬å°ºå¯¸ã€‚è¯¥ç±»æ¨¡å‹é€šå¸¸ä¹Ÿä¼šä½¿ç”¨ä¸€ä¸ªå°å‹CNNæå–å±€éƒ¨ç‰¹å¾å’Œä¿®æ”¹å°ºå¯¸

### æ¨¡å‹è¾“å‡ºæ ¼å¼

* æ–‡æœ¬è¾“å‡ºï¼šå’Œè¯­è¨€æ¨¡å‹ç›¸åŒï¼Œå°†decoderè¾“å‡ºçš„è¾“å‡ºåµŒå…¥å‘é‡é€šè¿‡ä¸€ä¸ªçº¿æ€§å¤´å’Œsofrmaxè½¬æ¢ä¸ºè¯æ±‡è¡¨ä¸­æ–‡æœ¬idçš„æ¦‚ç‡ï¼ˆä¹Ÿå°±æ˜¯**æœ€ç»ˆè¾“å‡ºç»´åº¦=è¯æ±‡è¡¨å¤§å°**ï¼‰
* ç›´æ¥æ³¢å½¢è¾“å‡ºï¼šæœ‰äº›æ¨¡å‹å¯ä»¥ç›´æ¥è¾“å‡ºæ³¢å½¢ï¼Œä½†è¾ƒå°‘
* æ—¶é¢‘è°±è¾“å‡ºï¼šå¯¹äºè¯¥ç±»æ¨¡å‹ï¼Œç”±äºæˆ‘ä»¬æœ€ç»ˆè¿˜æ˜¯éœ€è¦è¾“å‡ºä¸€ä¸ªæ³¢å½¢ï¼Œå› æ­¤é€šå¸¸æœ‰ä¸¤ç§åšæ³•ï¼š
  * åŸºäºistftï¼šå¦‚æœæˆ‘ä»¬è¾“å…¥æ¨¡å‹çš„æ—¶é¢‘è°±æ˜¯stftå¾—åˆ°çš„ï¼Œä¹Ÿå¯ä»¥é€šè¿‡istftè¿˜åŸã€‚ä½†æ­¤æ—¶æˆ‘ä»¬éœ€è¦çŸ¥é“**å¹…å€¼å’Œç›¸ä½**ä¸¤éƒ¨åˆ†çš„ä¿¡æ¯ï¼Œè€Œä¸€èˆ¬éŸ³é¢‘æ¨¡å‹è¾“å…¥ä»…ä½¿ç”¨åŸºäºå¹…å€¼ä¿¡æ¯çš„åŠŸç‡è°±ï¼Œå› æ­¤éœ€è¦ä¸€ä¸ªé¢å¤–çš„ç½‘ç»œä¼°è®¡å…¶ç›¸ä½ä¿¡æ¯ï¼ˆä¹Ÿæœ‰å…¶ä»–æ–¹æ³•ï¼Œè·Ÿæ¨¡å‹è¾“å…¥æœ‰å…³ï¼‰
  * åŸºäºç¥ç»ç½‘ç»œï¼šç›´æ¥å†ä½¿ç”¨ä¸€ä¸ªç¥ç»ç½‘ç»œå°†decoderè¾“å‡ºåµŒå…¥è½¬æ¢ä¸ºæ³¢å½¢ï¼ˆVocoderå£°ç å™¨ï¼‰

## ä¸¤ç§ç»“æ„

### CTCç»“æ„

CTCç»“æ„ï¼ˆConnectionist Temporal Classificationï¼‰æ˜¯ä¸€ç§**ä»…**ä½¿ç”¨Transformerç¼–ç å™¨ï¼ˆencoderï¼‰ç»“æ„çš„**è¯­éŸ³è¯†åˆ«**ï¼ˆASRï¼‰æ¨¡å‹ã€‚ä½¿ç”¨è¯¥æ¶æ„çš„æ¨¡å‹åŒ…æ‹¬Wav2Vec2ã€HuBERTã€M-CTC-Tç­‰ç­‰ã€‚

åœ¨CTCç»“æ„æ¨¡å‹ä¸­ï¼Œæ‰€ä½¿ç”¨çš„è¯æ±‡è¡¨é€šå¸¸æ˜¯å°è¯æ±‡è¡¨ï¼ˆå­—ç¬¦ã€éŸ³ç´ ç­‰ï¼‰ã€‚è¯¥ç±»æ¨¡å‹é€šå¸¸å°†è‹¥å¹²msçš„æ ·æœ¬åˆ‡ç‰‡è¾“å‡ºä¸ºä¸€ä¸ªtokenï¼Œç”±äºä¸€ä¸ªå­—æ¯å‘éŸ³å¯èƒ½åŒ…å«å¤šä¸ªåˆ‡ç‰‡ï¼Œ**æ¨¡å‹ä¾¿ä¼šè¾“å‡ºå¤šä¸ªé‡å¤å­—æ¯**ï¼Œå› ä¸ºæ¯ä¸ªtokenå¿…é¡»å¯¹åº”ä¸€ä¸ªç»“æœã€‚è€ŒCTCç®—æ³•å°±æ˜¯é€šè¿‡ä¸€ä¸ªç‰¹æ®Šæ ‡è®°ï¼ˆblank tokenï¼‰ï¼Œå‹ç¼©æ¨¡å‹è¾“å‡ºçš„é‡å¤æˆ–ç©ºç™½å†…å®¹

å¯¹äºç©ºç™½æ ‡è®°ï¼Œå…¶æ˜¯é€šè¿‡**ç‰¹æ®Šçš„æŸå¤±å‡½æ•°**è®©æ¨¡å‹å­¦ä¹ ä½•æ—¶è¯¥è¾“å‡ºç©ºç™½æ ‡è®°çš„ï¼Œå› æ­¤CTCæ¨¡å‹ä½¿ç”¨çš„lossä¸æ˜¯æ ‡å‡†çš„äº¤å‰ç†µã€‚**é™¤äº†è¯æ±‡è¡¨ä¸­æ·»åŠ ç©ºç™½æ ‡è®°ã€ä»…ä½¿ç”¨encoderå’Œä½¿ç”¨ç‰¹æ®Šçš„è®­ç»ƒç­–ç•¥ä¹‹å¤–**ï¼Œè¯¥ç±»æ¨¡å‹å°±æ²¡æœ‰ä»€ä¹ˆç‰¹æ®Šçš„ç‚¹äº†ã€‚

å¯¹äºåªè€ƒè™‘å•å­—ç¬¦çš„CTCæ¨¡å‹ï¼Œå¯èƒ½ä¼šè¾“å‡ºå¬èµ·æ¥æ­£ç¡®ä½†æ‹¼å†™ä¸æ­£ç¡®çš„å•è¯ï¼Œå› æ­¤å¯ä»¥ä½¿ç”¨**é¢å¤–çš„è¯­è¨€æ¨¡å‹**æ¥æé«˜éŸ³é¢‘çš„è½¬å½•è´¨é‡ã€‚è¿™ä¸ªè¯­è¨€æ¨¡å‹å®é™…ä¸Šæ˜¯ä½œä¸ºäº†CTCè¾“å‡ºçš„**æ‹¼å†™æ£€æŸ¥å™¨**ã€‚

### Seq2seqç»“æ„

æ¯”CTCç»“æ„çš„æ¨¡å‹èƒ½åŠ›æ›´å¼ºï¼Œä½¿ç”¨æ ‡å‡†çš„transformerç»“æ„ï¼Œä¸è¯­è¨€æ¨¡å‹åŸºæœ¬ä¸€è‡´ã€‚å› æ­¤å…¶æœ€ç»ˆçš„è¾“å‡ºå’Œè¯­è¨€æ¨¡å‹ä¸€æ ·ï¼Œéƒ½æ˜¯subwordï¼Œå¯¹äºwhisperï¼Œå…¶ä½¿ç”¨çš„å°±æ˜¯GPT2çš„åˆ†è¯å™¨ã€‚åœ¨ASRä»»åŠ¡ä¸­ï¼Œå…¶ä½¿ç”¨äº¤å‰ç†µä½œä¸ºæŸå¤±å‡½æ•°ã€‚

# ğŸ¤—Transformersåº“åŸºç¡€ğŸ¤—

## pipeline

transformeråº“ç›´æ¥è°ƒç”¨å·²ç¡®å®šå…·ä½“åŠŸèƒ½çš„æ¨¡å‹çš„å‡½æ•°

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
classifier(
    [
        "I've been waiting for a HuggingFace course my whole life.",
        "I hate this so much!",
    ]
)

>>>[{'label': 'POSITIVE', 'score': 0.9598047137260437},
>>> {'label': 'NEGATIVE', 'score': 0.9994558095932007}]
```

å¯¹äºä¸€ä¸ªè¯­è¨€æ¨¡å‹ï¼Œå…¶å¤§è‡´å¯ä»¥åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼Œå³tokenizerå’Œmodel

## tokenizer

### åŸç†

tokenizerçš„ä½œç”¨å°±æ˜¯å°†è¾“å…¥çš„æ–‡æœ¬è½¬åŒ–ä¸ºæ¨¡å‹å¯å¤„ç†çš„tensorï¼ˆ**å³ç¼–ç encode**ï¼‰ï¼ŒåŒæ—¶ç”Ÿæˆæ¯å¥è¯æ‰€å¯¹åº”çš„ä¸€äº›é¢å¤–ä¿¡æ¯ï¼Œå¦‚token_type_idså’Œattention_maskç­‰

åŸºæœ¬çš„tokenizerå¯ä»¥åˆ†ä¸ºä¸‰ç±»ï¼š

* word-basedï¼šç›´æ¥ä»¥ç©ºæ ¼åˆ†éš”å•è¯ï¼Œå°†æ¯ä¸ªå•è¯ï¼ˆwordï¼‰å¯¹åº”ä¸€ä¸ªå”¯ä¸€IDã€‚ç¼ºç‚¹å³æ˜¯éœ€è¦å¤§é‡çš„tokenåº“ï¼Œä»¥å­˜å‚¨æ‰€æœ‰å•è¯
* Character-basedï¼šç›´æ¥æ‹†ä¸ºå•ä¸ªå­—ç¬¦ï¼Œå°†æ¯ä¸ªå­—ç¬¦å¯¹åº”å”¯ä¸€IDã€‚ç¼ºç‚¹æ˜¯å¯¹äºä¸€ä¸ªå¥å­ï¼Œæ¨¡å‹éœ€è¦å¤„ç†å¤§é‡tokenï¼Œå¹¶ä¸”æ¯ä¸ªå­—ç¬¦æœ¬èº«æ²¡æœ‰å¤ªå¤§æ„ä¹‰ï¼ˆå¯¹äºè‹±æ–‡æ¥è¯´ï¼‰
* subwordï¼š**å‡ ä¹æ‰€æœ‰å¤§æ¨¡å‹éƒ½åœ¨ä½¿ç”¨çš„åˆ†è¯ç­–ç•¥**ï¼ŒåŸåˆ™ï¼šå¸¸ç”¨è¯ä¸åº”è¢«åˆ†è§£ä¸ºæ›´å°çš„å­è¯ï¼Œä½†ç½•è§è¯åº”è¢«åˆ†è§£ä¸ºæœ‰æ„ä¹‰çš„å­è¯ã€‚ä¾‹å¦‚ï¼Œå¯¹äº"tokenization"ï¼Œåˆ™å¯ä»¥è¢«åˆ†ä¸º"token"å’Œ"ization"ï¼Œå› ä¸ºè¿™ä¸¤ä¸ªsubwordå‡ºç°æ›´ä¸ºé¢‘ç¹ï¼Œå¹¶ä¸”è¿™æ ·åˆ†è¯ä¹Ÿå¯ä»¥ä¿ç•™å…¶æ„ä¹‰

æ¯ä¸ªæ¨¡å‹éƒ½æœ‰è‡ªå·±çš„å…·ä½“tokenizerä»¥å®ç°subwordç­–ç•¥

transformersä¾‹ç¨‹ï¼š

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")
```

tokenæ­¤æ—¶ä¸ºä¸€ä¸ªå­—å…¸ï¼ŒåŒ…æ‹¬äº†ç¼–ç å’Œæ³¨æ„åŠ›æ©ç ï¼ˆæŒ‡padding maskï¼‰

### å…·ä½“å®ç°

ç¼–ç åˆ†ä¸ºä¸¤æ­¥ï¼Œå³åˆ†è¯+è½¬åŒ–ä¸ºID

```python
#ä»…åˆ†è¯
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
sequence = "Using a Transformer network is simple"
tokens = tokenizer.tokenize(sequence)
>>>['Using', 'a', 'transform', '##er', 'network', 'is', 'simple']

#å°†åˆ†è¯è½¬ä¸ºID
ids = tokenizer.convert_tokens_to_ids(tokens)
print(ids)
>>>[7993, 170, 11303, 1200, 2443, 1110, 3014]

#è§£ç 
decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])
print(decoded_string)
>>>'Using a Transformer network is simple'
```

å¯¹äºä¸åŒçš„æ¨¡å‹ï¼Œå…¶è¿˜è¦æ±‚äº†å…¶ä»–çš„é¢å¤–è¾“å…¥ï¼Œå› æ­¤å…¶tokenizerä¹Ÿå…·æœ‰é™¤ç¼–ç å¤–çš„å…¶ä»–åŠŸèƒ½ï¼š

```python
# å°†å¥å­åºåˆ—å¡«å……åˆ°æœ€é•¿å¥å­çš„é•¿åº¦
model_inputs = tokenizer(sequences, padding="longest")

# å°†å¥å­åºåˆ—å¡«å……åˆ°æ¨¡å‹çš„æœ€å¤§é•¿åº¦
# (512 for BERT or DistilBERT)
model_inputs = tokenizer(sequences, padding="max_length")

# å°†å¥å­åºåˆ—å¡«å……åˆ°æŒ‡å®šçš„æœ€å¤§é•¿åº¦
model_inputs = tokenizer(sequences, padding="max_length", max_length=8)

# å°†æˆªæ–­æ¯”æ¨¡å‹æœ€å¤§é•¿åº¦é•¿çš„å¥å­åºåˆ—
# (512 for BERT or DistilBERT)
model_inputs = tokenizer(sequences, truncation=True)

# å°†æˆªæ–­é•¿äºæŒ‡å®šæœ€å¤§é•¿åº¦çš„å¥å­åºåˆ—
model_inputs = tokenizer(sequences, max_length=8, truncation=True)

# è¿”å› PyTorch tensors
model_inputs = tokenizer(sequences, padding=True, return_tensors="pt")

# è¿”å› TensorFlow tensors
model_inputs = tokenizer(sequences, padding=True, return_tensors="tf")

# è¿”å› NumPy arrays
model_inputs = tokenizer(sequences, padding=True, return_tensors="np")
```

## model

### åˆ›å»ºæ¨¡å‹

```python
from transformers import BertConfig, BertModel

config = BertConfig()
model = BertModel(config)
```

### åŠ è½½é¢„è®­ç»ƒæƒé‡

```python
model = BertModel.from_pretrained("bert-base-cased")

#æˆ–æ˜¯ç›´æ¥é€šè¿‡AutoModelç±»
checkpoint = "bert-base-cased"
model = AutoModel.from_pretrained(checkpoint)
```

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæ­¤å¤„çš„æ¨¡å‹è¾“å…¥éœ€å‚è€ƒå„æ¨¡å‹å…·ä½“è¦æ±‚ï¼Œä¸€èˆ¬ä¸ºä¸€ä¸ªå­—å…¸ï¼Œä¾‹å¦‚ï¼š

```
{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 			12172,     2607,  2026,  2878,  2166,  1012,   102],
        [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0]]), 
 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}
```

è¿™äº›å¯ä»¥è¢«æ¨¡å‹å¯¹åº”çš„tokenizerç›´æ¥ç”Ÿæˆï¼Œä»…éœ€ï¼š

```python
model(**tokens) 
"""
pythonçš„å­—å…¸è§£åŒ…æœºåˆ¶ï¼Œç­‰åŒäºmodel(
    input_ids=tensor(...),
    attention_mask=tensor(...)
"""
```

## dataset

```python
from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
raw_train_dataset = raw_datasets["train"]
sentence1 = raw_train_dataset['sentence1']
sample = raw_train_dataset[0]
```

### Datadict

è¯¥å‡½æ•°ä¼šè¿”å›ä¸€ä¸ª<u>DatasetDict</u>æ•°æ®ç»“æ„ï¼Œè¿™æ˜¯ä¸€ä¸ªç±»ä¼¼å­—å…¸çš„ç»“æ„

{% asset_img 1.png This is an image %} 

å¯¹å…¶è¿›è¡Œç´¢å¼•å¯ä»¥å¾—åˆ°å•ä¸ªæ•°æ®é›†ç»“æ„<u>Dataset</u>

### Dataset

å¯¹äºDatasetç»“æ„ï¼Œä¹Ÿå°±æ˜¯`raw_train_dataset`ï¼Œå…¶å­˜å‚¨æ–¹å¼æ˜¯åˆ—å¼å­˜å‚¨ï¼Œå¯¹äºæœ¬ä»£ç ï¼Œå…¶æ¯ä¸€åˆ—ä¸ºï¼š

```
'sentence1', 'sentence2', 'label', 'idx'
```

å¯¹è¿™äº›é”®åè¿›è¡Œç´¢å¼•å³å¯è¿”å›å•ä¸ª<u>Column</u>ç»“æ„

ä¹Ÿå¯ä»¥å¯¹å…¶è¿›è¡Œ**è¡Œç´¢å¼•**ï¼Œåˆ™ä¼šè¿”å›å•ä¸ªæ ·æœ¬çš„æ¯åˆ—ä¿¡æ¯ï¼ˆ**ä¸€ä¸ªæ ‡å‡†å­—å…¸**ï¼‰ï¼š

{% asset_img 2.png This is an image %} 

å…¶ä¸­ï¼Œå¯¹äºæœ¬ä»£ç æ¥è¯´ï¼Œlabelåˆ™æ˜¯ä»£è¡¨sentence1å’Œsentence2**æ˜¯å¦**åŒä¹‰

### è¿”å›tokençš„dataset

```python
def tokenize_function(examples):
    return tokenizer(
        examples["sentence1"],
        examples["sentence2"],
        truncation=True
    )
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
```

.mapæ–¹æ³•å¯ä»¥å°†æ¯ä¸ªæ–‡æœ¬ç»è¿‡tokenizationåçš„ç»“æœæ·»åŠ åˆ°åŸæœ¬çš„Datadictä¸­ï¼š

```pyhton
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 408
    })
    test: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 1725
    })
})
```

å½“ä¸éœ€è¦å…¶ä¸­çš„æŸäº›featuresæ—¶ï¼Œå¯ä»¥ï¼š

```python
samples = tokenized_datasets["train"][:]
samples = {k: v for k, v in samples.items() if k not in ["idx", "sentence1", "sentence2"]}
```

æˆ–è€…ä¹Ÿå¯ä»¥è°ƒç”¨Datasetdictçš„æ–¹æ³•ï¼š

```python
tokenized_datasets = tokenized_datasets.remove_columns(["sentence1", "sentence2", "idx"])
```

### åŠ¨æ€å¡«å……

åœ¨è®­ç»ƒ LLMæ—¶ï¼Œåªéœ€è¦å¯¹æ¯ä¸ª batch è¿›è¡Œ**åŠ¨æ€ padding**ï¼ˆå› ä¸ºæ¨¡å‹çš„è¾“å…¥å¿…é¡»æ˜¯è§„åˆ™çš„ï¼‰ï¼Œè€Œä¸æ˜¯å¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œç»Ÿä¸€ paddingï¼Œå› ä¸ºè¿™æ ·ä¼šå¤§é‡æµªè´¹è®¡ç®—èµ„æºã€‚

```python
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
samples = tokenized_datasets["train"][:8]
samples = {k: v for k, v in samples.items() if k not in ["idx", "sentence1", "sentence2"]}
batch = data_collator(samples)
```

å…¶ä¸­batchåˆ™æ˜¯nä¸ªæ ·æœ¬ï¼ˆä»£ç ä¸­ä¸º8ï¼‰æ‰€å¯¹åº”çš„ç»è¿‡tokenizaitonåçš„samplesï¼Œå¹¶ä¸”å…¶ä¸­çš„idå…¨éƒ¨éƒ½è¢«paddingè‡³8ä¸ªæ ·æœ¬ä¸­æœ€é•¿æ–‡æœ¬idçš„é•¿åº¦ï¼ˆbatch.item()å’Œsamplesçš„æ•°æ®ç»“æ„æ˜¯ç›¸åŒçš„ï¼Œéƒ½æ˜¯ä¸€ä¸ªæ ‡å‡†å­—å…¸ï¼Œä»…æœ‰idå‘ç”Ÿäº†paddingï¼‰

# æ¨¡å‹å¾®è°ƒ

## è°ƒç”¨Trainer API

```python
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification
from transformers import TrainingArguments, Trainer


raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
training_args = TrainingArguments("test-trainer")
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
)

trainer.train()
```

åœ¨ä½¿ç”¨`AutoModelForSequenceClassification`å®ä¾‹åŒ–æ¨¡å‹æ—¶ï¼Œä¼šæ”¶åˆ°ä¸€ä¸ªè­¦å‘Šï¼Œè¿™æ˜¯å› ä¸º BERT æ²¡æœ‰åœ¨å¥å­å¯¹åˆ†ç±»æ–¹é¢è¿›è¡Œè¿‡é¢„è®­ç»ƒï¼Œæ‰€ä»¥**é¢„è®­ç»ƒæ¨¡å‹çš„ head å·²ç»è¢«ä¸¢å¼ƒ**ï¼Œè€Œæ˜¯**æ·»åŠ äº†ä¸€ä¸ªé€‚åˆå¥å­åºåˆ—åˆ†ç±»çš„æ–°å¤´éƒ¨**ã€‚è¿™äº›è­¦å‘Šè¡¨æ˜ä¸€äº›æƒé‡æ²¡æœ‰ä½¿ç”¨ï¼ˆå¯¹åº”äºè¢«æ”¾å¼ƒçš„é¢„è®­ç»ƒå¤´çš„æƒé‡ï¼‰ï¼Œè€Œæœ‰äº›æƒé‡è¢«éšæœºåˆå§‹åŒ–ï¼ˆå¯¹åº”äºæ–° head çš„æƒé‡ï¼‰ã€‚

### è¯„ä¼°

å¯¹äºæ¨¡å‹çš„è¾“å‡ºï¼š

```python
predictions = trainer.predict(tokenized_datasets["validation"])
```

å…¶ä¼šè¿”å›ä¸€ä¸ªå…ƒç»„ï¼š`(predictions,label_ids,metrics)`ï¼›å…¶ä¸­predictionsä¸ºä¸€ä¸ªå½¢çŠ¶ä¸º(batchsize,cls_num)çš„äºŒç»´å¼ é‡ï¼Œç¬¬äºŒç»´åº¦ä¸ºæ¯ä¸ªæ ·æœ¬çš„logitsï¼Œå…¶ä¸­çš„æœ€å¤§å€¼åˆ™ä¸ºé¢„æµ‹ç»“æœï¼›label_idsä¸ºæ ·æœ¬çœŸå®æ ‡ç­¾ï¼Œmetricsä¸ºè‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡ï¼Œé»˜è®¤åªè¿”å›loss

å› æ­¤æˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸€ä¸ªè¯„ä¼°å‡½æ•°ï¼š

```python
def compute_metrics(eval_preds):
    metric = evaluate.load("glue", "mrpc")
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
```

å…¶ä¸­.computeæ–¹æ³•ä¼šè¿”å›å‡†ç¡®ç‡ä¸f1åˆ†æ•°

## ä¸ä½¿ç”¨Trainer

### ä½¿ç”¨torchçš„DataloaderåŠ è½½æ•°æ®é›†

```python
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

#åˆ å»ä¸éœ€è¦åˆ—ï¼Œå°†labelæ”¹åä¸ºlabelsï¼ˆæ¨¡å‹é»˜è®¤è¾“å…¥æ ¼å¼ï¼‰
tokenized_datasets = tokenized_datasets.remove_columns(["sentence1", "sentence2", "idx"])
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
tokenized_datasets.set_format("torch")
#>>>["attention_mask", "input_ids", "labels", "token_type_ids"]

from torch.utils.data import DataLoader
#Dataloaderå…¼å®¹transformersçš„DataCollatorWithPadding
train_dataloader = DataLoader(
    tokenized_datasets["train"], shuffle=True, batch_size=8, collate_fn=data_collator
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], batch_size=8, collate_fn=data_collator
)
```

æ­¤æ—¶dataloaderè¿”å›çš„æ¯ä¸ªbatchä¸ºï¼š

```
{'attention_mask': torch.Size([8, 65]),
 'input_ids': torch.Size([8, 65]),
 'labels': torch.Size([8]),
 'token_type_ids': torch.Size([8, 65])}
```

è¯¥batchå¯ä»¥ç›´æ¥è¾“å…¥åˆ°AutoModelForSequenceClassificationå®ä¾‹åŒ–çš„æ¨¡å‹ä¸­ï¼š

```python
from transformers import get_scheduler
import torch

optimizer = AdamW(model.parameters(), lr=5e-5)
num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(	#å­¦ä¹ ç‡çº¿æ€§è¡°å‡
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
model.to(device)

for epoch in range(num_epoch):
    model.train()
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
    
    model.eval()
	for batch in eval_loader:
        batch = {k: v.to(device) for k, v in batch.items()}
    	with torch.no_grad():
        	outputs = model(**batch)
        logits = outputs.logits
        ...
```

